{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing with AI library flake analysis\n",
    "* I tried to train and test flake analysis with the RHV dataset. I couldn't make sense of the results so I tried to deconstruct the code (remove engineering parts). In this notebook, I try to understand how flake analysis model is trained and understand the obtained results.\n",
    "* I bring all the code from extract.py, cluster.py, and ncd.py which makes this notebook bulky, but it's important in terms of debugging the code efficiently.\n",
    "* I don't bring in data.py because it is just engineering around data formats.\n",
    "* [AI Library code reference](https://gitlab.com/opendatahub/ai-library/-/tree/master/flakes_train/bots/learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import boto3\n",
    "import json\n",
    "import tempfile\n",
    "import gzip, pickle\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import io\n",
    "import uuid\n",
    "import argparse\n",
    "import subprocess\n",
    "import zipfile\n",
    "import json\n",
    "import re\n",
    "import operator\n",
    "import random\n",
    "import tempfile\n",
    "import sklearn.cluster\n",
    "import sklearn.neighbors\n",
    "import numpy\n",
    "import zlib\n",
    "import configparser\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extractor.py\n",
    "* This module is supposed to extract features from the text log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 3\n",
    "# This code extracts features from log items. In particular it normalizes\n",
    "# and exracts the log.\n",
    "#\n",
    "# TODO: We could weight log lines using TF-IDF, but that would require\n",
    "# a distance function that could apply that weight between lines. The\n",
    "# NCD distance we use cannot do that.\n",
    "\n",
    "import calendar\n",
    "import re\n",
    "import time\n",
    "\n",
    "import sklearn.feature_extraction.text\n",
    "\n",
    "# Ignore lines that appear in at least this fraction of logs\n",
    "IGNORE_THRESHHOLD = 0.6\n",
    "\n",
    "# Choose only one out of every N tracked items. These have\n",
    "# already been manually \"clustered\" elsewhere, and we only need\n",
    "# some cluster seeds\n",
    "TRACKER_SPARSE = 100\n",
    "\n",
    "# TODO: We should be able to detect these automatically and ignore them\n",
    "# But for now this is a pragmatic hack to reduce noise and processing time\n",
    "NOISE = {\n",
    "    \"Wrote file\": re.compile(\"Wrote.*\\.(png|html|log)\"),\n",
    "    \"Journal extracted\": re.compile(\"Journal extracted to.*\\.log\"),\n",
    "    \"Core dumps downloaded\": re.compile(\"Core dumps downloaded to.*\\.core\"),\n",
    "    \"not ok\": re.compile(\"^not ok.*\"),\n",
    "    \"ok\": re.compile(\"^ok.*\"),\n",
    "    \"# Flake\": re.compile(\"# Flake.*\"),\n",
    "    'File \"\\\\1\"': re.compile('File \"/[^\"]+/([^/]+)\"'),\n",
    "    \"### \": re.compile('#{3,80}\\s+'),\n",
    "}\n",
    "\n",
    "DIGITS = re.compile('\\d+')\n",
    "\n",
    "# Various features extracted\n",
    "FEATURE_LOG = 0                # string: The normalized and collapsed log extracted\n",
    "FEATURE_INDEX = 1              # number: Unique index of the item\n",
    "FEATURE_URL = 2                # string: The full URL to the test result\n",
    "FEATURE_NAME = 3               # string: The name of the test run\n",
    "FEATURE_CONTEXT = 4            # string: The context in which the test is run\n",
    "FEATURE_TRACKER = 5            # string: A tracker issue for this\n",
    "FEATURE_MERGED = 6             # number: 1 if merged, 0 if not, -1 if unknown\n",
    "FEATURE_TIMESTAMP = 7          # number: The time since epoch at which test was run\n",
    "\n",
    "# Return already tokenized data\n",
    "def noop(value):\n",
    "    return value\n",
    "\n",
    "# Select which items we want to operate on.\n",
    "#\n",
    "# Because we have so many tracked failures, we need to only bring\n",
    "# some of those into our clustering algorithm. We can assume that\n",
    "# these are already clusters\n",
    "tracked = { }\n",
    "def select(item):\n",
    "    if item.get(\"status\") != \"FAILED\":\n",
    "        return False\n",
    "    tracker = item.get(\"tracker\")\n",
    "    if not tracker:\n",
    "        return True\n",
    "    count = tracked[tracker] = tracked.get(tracker, 0) + 1\n",
    "    return count % TRACKER_SPARSE == 0 # Only every Nth for tracked failures\n",
    "\n",
    "# The actual feature extractor. Currently only extracts a\n",
    "# normalized log from each item. By using fit() you can train\n",
    "# the extractor to ignore frequently found lines.\n",
    "class Extractor():\n",
    "    def __init__(self, verbose=False):\n",
    "        self.extract = sklearn.feature_extraction.text.CountVectorizer(\n",
    "            analyzer='word',\n",
    "            tokenizer=noop,\n",
    "            lowercase=False,\n",
    "            max_df=IGNORE_THRESHHOLD)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(item):\n",
    "        result = [ ]\n",
    "        value = item[\"log\"] or \"\"\n",
    "        for line in value.replace('\\r\\n', '\\n').replace('\\r', '\\n').split('\\n'):\n",
    "            line = line.strip()\n",
    "            for substitute, pattern in NOISE.items():\n",
    "                line = pattern.sub(substitute, line)\n",
    "            else:\n",
    "                result.append(DIGITS.sub('000', line))\n",
    "        return result\n",
    "\n",
    "    def fit(self, items, tokenized=None):\n",
    "        tokenized = tokenized or map(Extractor.tokenize, items)\n",
    "        self.extract.fit(tokenized)\n",
    "\n",
    "    def transform(self, items, tokenized=None):\n",
    "        tokenized = list(tokenized or map(Extractor.tokenize, items))\n",
    "        results = [ ]\n",
    "        for index, item in enumerate(items):\n",
    "            if not select(item):\n",
    "                continue\n",
    "            lines = tokenized[index]\n",
    "            filtered = filter(lambda line: line not in self.extract.stop_words_, lines)\n",
    "#             try:\n",
    "#                 timestamp = calendar.timegm(time.strptime(item.get(\"date\", \"\"), \"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "#             except ValueError:\n",
    "            timestamp = -1\n",
    "            merged = item.get(\"merged\")\n",
    "            if merged is None:\n",
    "                merged = -1\n",
    "            else:\n",
    "                merged = merged and 1 or 0\n",
    "            results.append((\n",
    "                \"\\n\".join(filtered),      # FEATURE_LOG\n",
    "                index,                    # FEATURE_INDEX\n",
    "                item.get(\"url\", \"\"),      # FEATURE_URL\n",
    "                item.get(\"test\", \"\"),     # FEATURE_NAME\n",
    "                item.get(\"context\", \"\"),  # FEATURE_CONTEXT\n",
    "                item.get(\"tracker\", \"\"),  # FEATURE_TRACKER\n",
    "                merged,                   # FEATURE_MERGED\n",
    "                timestamp                 # FEATURE_TIMESTAMP\n",
    "            ))\n",
    "        return results\n",
    "\n",
    "    def fit_transform(self, items):\n",
    "        tokenized = list(map(Extractor.tokenize, items))\n",
    "        self.fit(items, tokenized)\n",
    "        return self.transform(items, tokenized)\n",
    "\n",
    "    def stop_tokens(self):\n",
    "        return self.extract.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ncd.py\n",
    "* This is the part where the log text is encoded into a compressed version and the distance between them is computed using a normalized compressed distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(val):\n",
    "    result = len(zlib.compress(val.encode('utf-8'), 6))\n",
    "    return float(result)\n",
    "\n",
    "# We use a cache to accelelerate NCD calculations and avoid\n",
    "# same of the re-compression of identical data\n",
    "cache = { }\n",
    "vectors = [ ]\n",
    "\n",
    "def calculate(a, b):\n",
    "    # Zero distance between identical pairs\n",
    "    if a == b:\n",
    "        return 0\n",
    "    # Compression length for individual parts are cached\n",
    "    Ka = cache.get(a)\n",
    "    if Ka is None:\n",
    "        Ka = K(a)\n",
    "    Kb = cache.get(b)\n",
    "    if Kb is None:\n",
    "        Kb = K(b)\n",
    "    # The compression distance for combined\n",
    "    Kab = K(a + b)\n",
    "    return (Kab - min(Ka, Kb)) / max(Ka, Kb)\n",
    "\n",
    "# Precompute all individual hashes to cache and convert to vector array\n",
    "def prepare(values):\n",
    "    global vectors, cache\n",
    "    values = list(values)\n",
    "    array = numpy.zeros((len(values), 1))\n",
    "    for i, value in enumerate(values):\n",
    "        index = len(vectors)\n",
    "        array[i][0] = index\n",
    "        vectors.append(value)\n",
    "        cache[value] = K(value)\n",
    "    return array\n",
    "\n",
    "# A function usable as a metric in scikit-learn\n",
    "# Make sure to call prepare() first on the actual values\n",
    "def metric(x, y):\n",
    "    global vectors\n",
    "    i, j = int(x[0]), int(y[0])\n",
    "    return calculate(vectors[i], vectors[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster.py\n",
    "## Relevant parts of class Cluster\n",
    "* The cluster class is basically methods for analyzing the clusters formed by the DBSCAN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 5\n",
    "FLAKE_THRESHOLD = 0.4\n",
    "\n",
    "# A cluster of items with optional analysis of those items\n",
    "# The items are not stored here, but their points in the\n",
    "# array are.\n",
    "class Cluster():\n",
    "    def __init__(self, label, points):\n",
    "        self.label = label\n",
    "        self.points = points\n",
    "\n",
    "    # Analyse the cluster, based on the points added in\n",
    "    # the cluster. The points should be indexes into the\n",
    "    # items array.\n",
    "    def analyze(self, features):\n",
    "        num_merged = 0\n",
    "\n",
    "        for point in self.points:\n",
    "            merged = features[point][FEATURE_MERGED]\n",
    "            if merged == 1:\n",
    "                num_merged += 1\n",
    "\n",
    "        total = len(self.points)\n",
    "\n",
    "        # Calculate the merged probabilities\n",
    "        if total:\n",
    "            merged = (float(num_merged) / float(total))\n",
    "            if merged > 1:\n",
    "                merged = 1\n",
    "\n",
    "        # Probability that this cluster represents the given name\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"merged\": merged,\n",
    "            \"trackers\": self.group_by(features, FEATURE_TRACKER, factor=TRACKER_SPARSE),\n",
    "            \"names\": self.group_by(features, FEATURE_NAME),\n",
    "            \"contexts\": self.group_by(features, FEATURE_CONTEXT)\n",
    "        }\n",
    "\n",
    "    # Figure out how often given values of a feature show up in a cluster\n",
    "    def group_by(self, features, feature, limit=5, factor=1):\n",
    "        values = { }\n",
    "        total = 0\n",
    "        for point in self.points:\n",
    "            value = features[point][feature]\n",
    "            if value:\n",
    "                # If we have a factor, some of the features may be sparse\n",
    "                # So account for the spareness in our probability estimates\n",
    "                values[value] = values.get(value, 0) + factor\n",
    "                total += factor\n",
    "            else:\n",
    "                total += 1\n",
    "        listing = [ ]\n",
    "        for value, count in values.items():\n",
    "            probability = float(count) / float(total or 1)\n",
    "            listing.append((value, min(probability, 1)))\n",
    "        listing.sort(key=operator.itemgetter(1), reverse=True)\n",
    "        return listing[0:limit]\n",
    "\n",
    "    # Dump the selected cluster to disk. The features are the inputs\n",
    "    # from the model that were used to build the cluster.\n",
    "    def dump(self, directory, features, detail=None):\n",
    "        if self.label is None:\n",
    "            label = \"noise\"\n",
    "        else:\n",
    "            label = \"cluster-{0}\".format(self.label)\n",
    "\n",
    "        # Dump our stuff into the directory\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "\n",
    "        path = os.path.join(directory, \"{0}-{1}.log\".format(label, detail or len(self.points)))\n",
    "        with open(path, \"a\") as fp:\n",
    "            for row in self.analyze(features).items():\n",
    "                fp.write(\"{0}: {1}\\n\".format(row[0], repr(row[1])))\n",
    "            fp.write(\"\\n\\n\")\n",
    "            for point in self.points:\n",
    "                url = features[point][extractor.FEATURE_URL]\n",
    "                if url:\n",
    "                    fp.write(\"{0}\\n\".format(url))\n",
    "                fp.write(features[point][extractor.FEATURE_LOG])\n",
    "                fp.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant parts of class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The clustering model. Uses unsupervised clustering to build clusters\n",
    "# out of data extracted from test logs. See extractor.py for the code\n",
    "# that extracts features from the logs.\n",
    "#\n",
    "# Also allows classification into the built clusters.\n",
    "#\n",
    "class Model():\n",
    "\n",
    "    def __init__(self, verbose=False, eps=0.3, min_samples=3):\n",
    "        self.clusters = { }      # The actual clustered items\n",
    "        self.verbose = verbose\n",
    "        self.extractor = None\n",
    "        self.features = None\n",
    "        self.eps = eps          # Maximum distance between two samples in neighborhood\n",
    "        self.min_samples = min_samples     # Minimum number of samples in a cluster\n",
    "\n",
    "    # Perform the unsupervised clustering\n",
    "    def train(self, items):\n",
    "        self.clusters = { }\n",
    "        self.noise = [ ]\n",
    "\n",
    "        items = list(items)\n",
    "\n",
    "        if self.verbose:\n",
    "            sys.stderr.write(\"{0}: Items to train\\n\".format(len(items)))\n",
    "\n",
    "        # Extract the features we want to use for clustering from the items\n",
    "        self.extractor = Extractor()\n",
    "        self.features = self.extractor.fit_transform(items)\n",
    "\n",
    "        jobs = -1\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # Initialize the NCD code with our log feature. Currently only\n",
    "        # one feature is used: the normalized log\n",
    "        self.X = prepare(map(lambda features: features[FEATURE_LOG], self.features))\n",
    "        \n",
    "        #@s This calls ncd.py which has cache in global scope, check\n",
    "        self.matrix = sklearn.metrics.pairwise.pairwise_distances(self.X, metric=metric, n_jobs=jobs)\n",
    "\n",
    "        if self.verbose:\n",
    "            sys.stderr.write(\"{0}: Computed distances in {1} seconds on {2} cores\\n\".format(\n",
    "                int((len(self.features) * len(self.features)) / 2),\n",
    "                int(time.perf_counter() - start), jobs\n",
    "            ))\n",
    "\n",
    "        # Actually perform the clustering. This is fast compared to above\n",
    "        min_samples = min(self.min_samples, len(self.features) / 10)\n",
    "        dbs = sklearn.cluster.DBSCAN(metric='precomputed', eps=self.eps, min_samples=self.min_samples)\n",
    "        dbs.fit(self.matrix)\n",
    "        labels = dbs.labels_\n",
    "\n",
    "        # Create clusters of all the items\n",
    "        clusters = { }\n",
    "        noise = [ ]\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == -1:\n",
    "                noise.append(i)\n",
    "            else:\n",
    "                if label not in clusters:\n",
    "                    clusters[label] = [ ]\n",
    "                clusters[label].append(i)\n",
    "        self.clusters = { }\n",
    "        for label, indexes in clusters.items():\n",
    "            self.clusters[label] = Cluster(label, indexes)\n",
    "        self.noise = Cluster(None, noise)\n",
    "\n",
    "        # Print out a rough description of that\n",
    "        if self.verbose:\n",
    "            sys.stderr.write(\"{0}: Clusters ({1} items, {2} noise)\\n\".format(\n",
    "                len(self.clusters.keys()),\n",
    "                len(self.features) - len(noise),\n",
    "                len(noise)\n",
    "            ))\n",
    "\n",
    "        # Setup our neighbors classifier for predict()\n",
    "        self.neighbors = sklearn.neighbors.KNeighborsClassifier(metric='precomputed', weights='distance')\n",
    "        self.neighbors.fit(self.matrix, labels)\n",
    "\n",
    "    # Predict which clusters these items are a part of\n",
    "    # The cluster labels are returned for each item, along with a probability\n",
    "    def predict(self, items):\n",
    "        features = self.extractor.transform(items)\n",
    "        Y = prepare(map(lambda x: x[0], self.features))\n",
    "        X = prepare(map(lambda x: x[0], features))\n",
    "        matrix = sklearn.metrics.pairwise.pairwise_distances(X, Y, metric=metric, n_jobs=1)\n",
    "        result = [ ]\n",
    "\n",
    "        # TODO: The probability is currently bogus, we could use distance measurements to fill it in\n",
    "        for label in self.neighbors.predict(matrix):\n",
    "            if label == -1:\n",
    "                result.append((None, 0.0))\n",
    "            else:\n",
    "                # TODO: The problem here is we don't classify noise properly, should use eps (above)\n",
    "                result.append((label, 0.5))\n",
    "        return result\n",
    "\n",
    "    # Dump the cluster's models and noise to a directory\n",
    "    def dump(self, directory):\n",
    "        for label, cluster in self.clusters.items():\n",
    "            cluster.dump(directory, self.features)\n",
    "        self.noise.dump(directory, self.features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a dummy dataset\n",
    "* I design a dataset (same format as a real dataset) with 20 items\n",
    "* Each item has a log message randomly chosen from 3 sentences\n",
    "* These messages are very simple and short sentences like \"The test failed because of bad code\" \n",
    "* The simple examples help us understand the process but may not reflect actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\"The test failed because of bad code\", \"This is system error\", \"These are some kernel warinings\"]\n",
    "\n",
    "\n",
    "training_data = []\n",
    "for i in range(20):\n",
    "    item = {\"id\":\"041f6832-aa14-4f6e-891d-31aaf8d7ed01\",\n",
    "        \"status\":\"FAILED\",\n",
    "        \"pull\":7331,\n",
    "        \"log\":random.choice(messages),\n",
    "        \"test\":\"testFormatTypes (check_storage_format.TestStorage)\",\n",
    "        \"context\":\"verify/debian-testing\",\n",
    "        \"date\":-1,\n",
    "        \"merged\":True,\n",
    "        \"revision\":\"b32635869b9e87cdd9e42b6e6123150d500f6862\"}\n",
    "    training_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20: Items to train\n",
      "200: Computed distances in 0 seconds on -1 cores\n",
      "3: Clusters (20 items, 0 noise)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL\n",
    "#import learn.cluster\n",
    "#items = list(learn.data.load(inp, only=learn.data.failures, verbose=True))\n",
    "model = Model(verbose=True)\n",
    "model.train(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the training process\n",
    "* We have 3 clusters trained \n",
    "* Next, we try to understand how we got them\n",
    "* The first part of the train function in the class Model is exctracting features using class Extractor\n",
    "* Printing model.features\n",
    "* Note that the current implementation only uses log messages as input feature and the rest of the feature columns are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_message</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The test failed because of bad code</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The test failed because of bad code</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>These are some kernel warinings</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>These are some kernel warinings</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The test failed because of bad code</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The test failed because of bad code</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>These are some kernel warinings</td>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>These are some kernel warinings</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>These are some kernel warinings</td>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The test failed because of bad code</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This is system error</td>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td>testFormatTypes (check_storage_format.TestStor...</td>\n",
       "      <td>verify/debian-testing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            log_message   1 2  \\\n",
       "0   The test failed because of bad code   0     \n",
       "1   The test failed because of bad code   1     \n",
       "2                  This is system error   2     \n",
       "3                  This is system error   3     \n",
       "4                  This is system error   4     \n",
       "5                  This is system error   5     \n",
       "6                  This is system error   6     \n",
       "7                  This is system error   7     \n",
       "8       These are some kernel warinings   8     \n",
       "9       These are some kernel warinings   9     \n",
       "10                 This is system error  10     \n",
       "11  The test failed because of bad code  11     \n",
       "12  The test failed because of bad code  12     \n",
       "13      These are some kernel warinings  13     \n",
       "14      These are some kernel warinings  14     \n",
       "15                 This is system error  15     \n",
       "16                 This is system error  16     \n",
       "17      These are some kernel warinings  17     \n",
       "18  The test failed because of bad code  18     \n",
       "19                 This is system error  19     \n",
       "\n",
       "                                                    3                      4  \\\n",
       "0   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "1   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "2   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "3   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "4   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "5   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "6   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "7   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "8   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "9   testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "10  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "11  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "12  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "13  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "14  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "15  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "16  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "17  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "18  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "19  testFormatTypes (check_storage_format.TestStor...  verify/debian-testing   \n",
       "\n",
       "   5  6  7  \n",
       "0     1 -1  \n",
       "1     1 -1  \n",
       "2     1 -1  \n",
       "3     1 -1  \n",
       "4     1 -1  \n",
       "5     1 -1  \n",
       "6     1 -1  \n",
       "7     1 -1  \n",
       "8     1 -1  \n",
       "9     1 -1  \n",
       "10    1 -1  \n",
       "11    1 -1  \n",
       "12    1 -1  \n",
       "13    1 -1  \n",
       "14    1 -1  \n",
       "15    1 -1  \n",
       "16    1 -1  \n",
       "17    1 -1  \n",
       "18    1 -1  \n",
       "19    1 -1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Printing the model features \n",
    "pd.DataFrame(model.features).rename(columns={0:'log_message'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The result is not a matrix of numbers as would be expected in count vectorizer\n",
    "* Count vectorizer is trained (fit) but never used as a transformer (see class Extractor in this notebook)\n",
    "* Is the point of extractor.py just to select \"FAILED\" cases and remove stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving ahead\n",
    "* The next steps in the train function is to use code from ncd.py \n",
    "* The prepare and metric functions are two important functions to note.\n",
    "* This returns self.X and self.matrix which is used for training DBSCAN\n",
    "* Printing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.],\n",
       "       [11.],\n",
       "       [12.],\n",
       "       [13.],\n",
       "       [14.],\n",
       "       [15.],\n",
       "       [16.],\n",
       "       [17.],\n",
       "       [18.],\n",
       "       [19.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.74418605, 0.74418605, 0.74418605,\n",
       "        0.74418605, 0.74418605, 0.74418605, 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.        , 0.        , 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.74418605, 0.60465116, 0.        , 0.74418605],\n",
       "       [0.        , 0.        , 0.74418605, 0.74418605, 0.74418605,\n",
       "        0.74418605, 0.74418605, 0.74418605, 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.        , 0.        , 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.74418605, 0.60465116, 0.        , 0.74418605],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.60465116, 0.60465116, 0.74358974, 0.74358974, 0.74358974,\n",
       "        0.74358974, 0.74358974, 0.74358974, 0.        , 0.        ,\n",
       "        0.74358974, 0.60465116, 0.60465116, 0.        , 0.        ,\n",
       "        0.74358974, 0.74358974, 0.        , 0.60465116, 0.74358974],\n",
       "       [0.60465116, 0.60465116, 0.74358974, 0.74358974, 0.74358974,\n",
       "        0.74358974, 0.74358974, 0.74358974, 0.        , 0.        ,\n",
       "        0.74358974, 0.60465116, 0.60465116, 0.        , 0.        ,\n",
       "        0.74358974, 0.74358974, 0.        , 0.60465116, 0.74358974],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.        , 0.        , 0.74418605, 0.74418605, 0.74418605,\n",
       "        0.74418605, 0.74418605, 0.74418605, 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.        , 0.        , 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.74418605, 0.60465116, 0.        , 0.74418605],\n",
       "       [0.        , 0.        , 0.74418605, 0.74418605, 0.74418605,\n",
       "        0.74418605, 0.74418605, 0.74418605, 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.        , 0.        , 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.74418605, 0.60465116, 0.        , 0.74418605],\n",
       "       [0.60465116, 0.60465116, 0.74358974, 0.74358974, 0.74358974,\n",
       "        0.74358974, 0.74358974, 0.74358974, 0.        , 0.        ,\n",
       "        0.74358974, 0.60465116, 0.60465116, 0.        , 0.        ,\n",
       "        0.74358974, 0.74358974, 0.        , 0.60465116, 0.74358974],\n",
       "       [0.60465116, 0.60465116, 0.74358974, 0.74358974, 0.74358974,\n",
       "        0.74358974, 0.74358974, 0.74358974, 0.        , 0.        ,\n",
       "        0.74358974, 0.60465116, 0.60465116, 0.        , 0.        ,\n",
       "        0.74358974, 0.74358974, 0.        , 0.60465116, 0.74358974],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ],\n",
       "       [0.60465116, 0.60465116, 0.74358974, 0.74358974, 0.74358974,\n",
       "        0.74358974, 0.74358974, 0.74358974, 0.        , 0.        ,\n",
       "        0.74358974, 0.60465116, 0.60465116, 0.        , 0.        ,\n",
       "        0.74358974, 0.74358974, 0.        , 0.60465116, 0.74358974],\n",
       "       [0.        , 0.        , 0.74418605, 0.74418605, 0.74418605,\n",
       "        0.74418605, 0.74418605, 0.74418605, 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.        , 0.        , 0.60465116, 0.60465116,\n",
       "        0.74418605, 0.74418605, 0.60465116, 0.        , 0.74418605],\n",
       "       [0.74418605, 0.74418605, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.74358974, 0.74358974,\n",
       "        0.        , 0.74418605, 0.74418605, 0.74358974, 0.74358974,\n",
       "        0.        , 0.        , 0.74358974, 0.74418605, 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding prepare and other operations in ncd.py\n",
    "* self.X is not feature matrix for the model or count vectorizer matrix like I thought, it is a cached set in ncd.py. Values of self.X corresponds to keys of cache in ncd.py\n",
    "* The values in cache in ncd.py is the length of compressed string (see function ncd.K in this notebook)\n",
    "* Now, the normalized difference in the compressed values are used as distance for clustering \n",
    "* Printing cache and calculation of k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The test failed because of bad code': 43.0,\n",
       " 'This is system error': 26.0,\n",
       " 'These are some kernel warinings': 39.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The test failed because of bad code',\n",
       " 'The test failed because of bad code',\n",
       " 'This is system error',\n",
       " 'This is system error',\n",
       " 'This is system error',\n",
       " 'This is system error',\n",
       " 'This is system error',\n",
       " 'This is system error',\n",
       " 'These are some kernel warinings',\n",
       " 'These are some kernel warinings',\n",
       " 'This is system error',\n",
       " 'The test failed because of bad code',\n",
       " 'The test failed because of bad code',\n",
       " 'These are some kernel warinings',\n",
       " 'These are some kernel warinings',\n",
       " 'This is system error',\n",
       " 'This is system error',\n",
       " 'These are some kernel warinings',\n",
       " 'The test failed because of bad code',\n",
       " 'This is system error']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7441860465116279"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we print a demo calculation of distance\n",
    "# between two input logs.\n",
    "calculate(vectors[0], vectors[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model results\n",
    "\n",
    "* It seems to work for this simple example as we just have 3 one dimension vectors.\n",
    "* We get three clusters with same messages clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The test failed because of bad code', 1.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusters[0].group_by(model.features, FEATURE_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 11, 12, 18]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusters[0].points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This is system error', 1.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusters[1].group_by(model.features, FEATURE_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 10, 15, 16, 19]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusters[1].points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('These are some kernel warinings', 1.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusters[2].group_by(model.features, FEATURE_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 13, 14, 17]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusters[2].points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's try with a real life dataset of complex logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RHV dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"./config.txt\")\n",
    "secret = config.get(\"DH-PLAYPEN\",\"secret\")\n",
    "key = config.get(\"DH-PLAYPEN\",\"key\")\n",
    "\n",
    "# SET PARAMETERS TO ACCESS S3 BACKEND\n",
    "s3Path = 'ccit'\n",
    "s3Destination = 'ccit/model1.model'\n",
    "s3endpointUrl = 'https://s3.upshift.redhat.com/'\n",
    "s3objectStoreLocation = 'DH-PLAYPEN'\n",
    "s3accessKey = key\n",
    "s3secretKey = secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CONNECTION TO S3 BACKEND\n",
    "session = boto3.Session(aws_access_key_id=s3accessKey,\n",
    "                        aws_secret_access_key=s3secretKey)\n",
    "s3 = session.resource('s3', endpoint_url=s3endpointUrl, verify=False)\n",
    "\n",
    "bucket = s3.Bucket(name=s3objectStoreLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD TRAINING DATA\n",
    "\n",
    "# get list of all availble objects\n",
    "objects = []\n",
    "for obj in bucket.objects.filter(Prefix=s3Path):\n",
    "    objects.append(obj.key)\n",
    "    \n",
    "count_both = 0\n",
    "training_data = []\n",
    "for key in objects:\n",
    "    obj = s3.Object(s3objectStoreLocation, key)\n",
    "    contents = obj.get()['Body'].read().decode('utf-8')\n",
    "    if contents:\n",
    "        jcontents = json.loads(contents)\n",
    "        if jcontents[\"log\"] != \"[]\" and jcontents[\"status\"] == \"FAILED\":\n",
    "            training_data.append((key[5:], jcontents))\n",
    "            count_both += 1\n",
    "            print(count_both)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We get 492 log files that can be used for training and testing.\n",
    "* First, we store them in another bucket so that we don't have to repeat above cell again and again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save useful data on shanand/ccit\n",
    "for tdata in training_data:\n",
    "    obj = s3.Object(s3objectStoreLocation, f'shanand/ccit_26Nov2020/training/{tdata[0]}.json')\n",
    "    obj.put(Body=json.dumps(tdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required data from shanand/ccit\n",
    "objects=[]\n",
    "for obj in bucket.objects.filter(Prefix=\"shanand/ccit_26Nov2020\"):\n",
    "    objects.append(obj.key)\n",
    "training_data = []\n",
    "for key in objects:\n",
    "    print(key)\n",
    "    obj = s3.Object(s3objectStoreLocation, key)\n",
    "    contents = obj.get()['Body'].read().decode('utf-8')\n",
    "    if contents:\n",
    "        jcontents = json.loads(contents)\n",
    "        training_data.append(jcontents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Randomly select a subset of 50 logs to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "training_data_sample = random.sample(training_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count the actual flakes in the dataset\n",
    "def count_merged(data): \n",
    "    count = 0\n",
    "    for record in data:\n",
    "        if record[1]['merged']:\n",
    "            count += 1\n",
    "    return(count)\n",
    "count_merged(training_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50: Items to train\n",
      "1250: Computed distances in 133 seconds on -1 cores\n",
      "0: Clusters (0 items, 50 noise)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL\n",
    "cache = { }\n",
    "vectors = [ ]\n",
    "model = Model(verbose=True)\n",
    "model.train([i[1] for i in training_data_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This says that all of these points are noises.\n",
    "* The tool will give no predictions for this dataset with current the hardcoded hyperparameters.\n",
    "* Let's try training with logs with \"RHV-4.4-tier3\" as prefix.\n",
    "* Training with one type of log may eliminate any noise obtained from logs from different types of logs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "cr = 0\n",
    "training_data_sample_2 = []\n",
    "for i in training_data:\n",
    "    if i[0].startswith('RHV-4.4-tier3'):\n",
    "        training_data_sample_2.append(i)\n",
    "        cr+=1\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count the number of flakes\n",
    "count_merged(training_data_sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63: Items to train\n",
      "1984: Computed distances in 558 seconds on -1 cores\n",
      "0: Clusters (0 items, 63 noise)\n"
     ]
    }
   ],
   "source": [
    "cache = { }\n",
    "vectors = [ ]\n",
    "model = Model(verbose=True)\n",
    "model.train([i[1] for i in training_data_sample_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This clustered 0 items and labelled 63 as noise\n",
    "* Clearly, we need to change the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50: Items to train\n",
      "1250: Computed distances in 134 seconds on -1 cores\n",
      "0: Clusters (0 items, 50 noise)\n"
     ]
    }
   ],
   "source": [
    "cache = { }\n",
    "vectors = [ ]\n",
    "model = Model(verbose=True, eps=0.8, min_samples=2)\n",
    "model.train([i[1] for i in training_data_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Still getting noise\n",
    "* Let's increase eps to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50: Items to train\n",
      "1250: Computed distances in 128 seconds on -1 cores\n",
      "1: Clusters (50 items, 0 noise)\n"
     ]
    }
   ],
   "source": [
    "cache = { }\n",
    "vectors = [ ]\n",
    "model = Model(verbose=True, eps=1, min_samples=2)\n",
    "model.train([i[1] for i in training_data_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we see everything clustered in one cluster.\n",
    "* It could mean that the vectors are not able to find meaningful seperation.\n",
    "* Let's try to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f14d01e8d30>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "vector_values = list(cache.values())\n",
    "y = [0 for i in vector_values]\n",
    "plt.scatter(list(cache.values()), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So as we can see, there are three logs that are different from the rest of the logs.\n",
    "* Since the minimum number of logs needed to form a cluster is 2, the three logs are either clustered with all the logs (1 cluster of 50 logs) or none of them are clustered together (all labelled as noise)\n",
    "* Overall, it shows that the one dimension encoding is not able to capture the seperation between the logs.\n",
    "* Let's confirm this by training with the complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the entire data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find total number of flakes\n",
    "count_merged(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "492: Items to train\n",
      "121032: Computed distances in 18235 seconds on -1 cores\n",
      "1: Clusters (3 items, 489 noise)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL\n",
    "cache = { }\n",
    "vectors = [ ]\n",
    "model = Model(verbose=True)\n",
    "model.train([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we see 1 cluster with 3 points and the rest classified as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the merged value of the clustered points\n",
    "model.clusters[0].group_by(model.features, FEATURE_MERGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see all of them were merged. Therefore this cluster has all flake test logs.\n",
    "* Next, we see the classification report on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80       327\n",
      "           1       1.00      0.02      0.04       165\n",
      "\n",
      "    accuracy                           0.67       492\n",
      "   macro avg       0.83      0.51      0.42       492\n",
      "weighted avg       0.78      0.67      0.54       492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def get_merged(data): \n",
    "    merged = []\n",
    "    for record in data:\n",
    "        if record[1]['merged']:\n",
    "            merged.append(1)\n",
    "        else:\n",
    "            merged.append(0)\n",
    "    return(merged)\n",
    "y_true = get_merged(training_data)\n",
    "y_pred = [1 if e in model.clusters[0].points else 0 for e,i in enumerate(training_data)]\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* Only 3 out of 165 flakes were identified using this method and the f1-score on the training set is 0.04 for the flake class. \n",
    "* If we keep the original hardcoded hyperparameters, i.e., eps=0.3, min samples=3, most logs are determined as noise. It would default to not a flake. We have around ~30% flakes in our dataset implying the accuracy of this model would be ~70%. However, f1-score for the flake class would be close to 0. \n",
    "* If we modify the hyperparameters, everything will become one cluster. Since most points in this cluster will not be flakes, the cluster would be defined as non flakes. Again we get ~70% accuracy and ~0 f1-score for flake class.\n",
    "    \n",
    "* In essence, the current method works well for very simple log messages but we will need to apply our data science workflow (EDA, research, POC) for real and complex logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
